{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Exploration","metadata":{"id":"8AvFrxTUlVoV"}},{"cell_type":"markdown","source":"## Installing dependencies","metadata":{}},{"cell_type":"code","source":"DEPENDENCIES = [\n    'tf-slim==1.1.0',\n    'numpy==1.21.6',\n    'pandas==1.3.5',\n    'seaborn',\n    'torch==1.11.0+cpu',\n    'torchvision==0.12.0+cpu',\n    'matplotlib==3.5.3',\n    'opencv-python==4.5.4.60',\n    'sklearn==0.0.post1',\n    'skorch==0.12.1',\n    'tqdm',\n    'requests',\n    'plotly==5.11.0',\n    'scikit-image==0.19.3',\n]","metadata":{"execution":{"iopub.status.busy":"2023-01-12T09:53:58.851599Z","iopub.execute_input":"2023-01-12T09:53:58.852294Z","iopub.status.idle":"2023-01-12T09:53:58.942266Z","shell.execute_reply.started":"2023-01-12T09:53:58.852154Z","shell.execute_reply":"2023-01-12T09:53:58.940043Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import subprocess\nimport typing as tp\nimport re\n\ndef install_dependencies(dependencies: tp.List[str], show_progress: bool = True) -> tp.Tuple[tp.List[str], tp.List[Exception]]:\n    emit = print if show_progress else lambda x: None\n\n    resolved_dependencies, errors = [], []\n    for dependency in dependencies:\n        emit(f'Installing \"{dependency}\"...')\n\n        try:\n            subprocess.run([\"pip\", \"install\", \"--root-user-action=ignore\", dependency], stdout=subprocess.DEVNULL)\n            \n            if '==' in dependency:\n                dependency = re.search('(.+)==.+', dependency).group(1)\n\n            if '@' in dependency:\n                dependency = re.search('(.+) @ .+', dependency).group(1)\n            \n            pip_freeze = subprocess.Popen((\"pip\", \"freeze\"), stdout=subprocess.PIPE)\n            output = subprocess.check_output((\"grep\", \"-E\", f\"^({dependency}==)|({dependency} @).+$\"), stdin=pip_freeze.stdout)\n            resolved_dependencies.append(output.decode().strip())\n        except subprocess.CalledProcessError as e:\n            errors.append(e)\n    \n    return resolved_dependencies, errors","metadata":{"execution":{"iopub.status.busy":"2023-01-12T09:53:58.944773Z","iopub.execute_input":"2023-01-12T09:53:58.945499Z","iopub.status.idle":"2023-01-12T09:53:58.956867Z","shell.execute_reply.started":"2023-01-12T09:53:58.945455Z","shell.execute_reply":"2023-01-12T09:53:58.955728Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"install_dependencies(DEPENDENCIES)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T09:53:58.959078Z","iopub.execute_input":"2023-01-12T09:53:58.959995Z","iopub.status.idle":"2023-01-12T09:57:06.325979Z","shell.execute_reply.started":"2023-01-12T09:53:58.959876Z","shell.execute_reply":"2023-01-12T09:57:06.324535Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Installing \"tf-slim==1.1.0\"...\nInstalling \"numpy==1.21.6\"...\nInstalling \"pandas==1.3.5\"...\nInstalling \"seaborn\"...\nInstalling \"torch==1.11.0+cpu\"...\nInstalling \"torchvision==0.12.0+cpu\"...\nInstalling \"matplotlib==3.5.3\"...\nInstalling \"opencv-python==4.5.4.60\"...\nInstalling \"sklearn==0.0.post1\"...\nInstalling \"skorch==0.12.1\"...\nInstalling \"tqdm\"...\nInstalling \"requests\"...\nInstalling \"plotly==5.11.0\"...\nInstalling \"scikit-image==0.19.3\"...\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(['tf-slim==1.1.0',\n  'numpy==1.21.6',\n  'pandas==1.3.5',\n  'seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-split_1629095986539/work',\n  'torch==1.11.0+cpu',\n  'torchvision==0.12.0+cpu',\n  'matplotlib==3.5.3',\n  'opencv-python==4.5.4.60',\n  'sklearn==0.0.post1',\n  'skorch==0.12.1',\n  'tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1649051611147/work',\n  'requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work',\n  'plotly==5.11.0',\n  'scikit-image==0.19.3'],\n [])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nBASE_DIR = Path.cwd()\nINPUT_DIR = Path(\"/\") / \"kaggle\" / \"input\"\nDATA_DIR = INPUT_DIR / \"planets-dataset\" / \"planet\" / \"planet\" # https://www.kaggle.com/datasets/nikitarom/planets-dataset\n\nTRAIN_SAMPLES_DIR = DATA_DIR / 'train-jpg'\nTRAIN_LABELS_FILE = DATA_DIR / 'train_classes.csv' ","metadata":{"execution":{"iopub.status.busy":"2023-01-12T09:57:06.329112Z","iopub.execute_input":"2023-01-12T09:57:06.329562Z","iopub.status.idle":"2023-01-12T09:57:06.336689Z","shell.execute_reply.started":"2023-01-12T09:57:06.329518Z","shell.execute_reply":"2023-01-12T09:57:06.335417Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import joblib\nimport matplotlib.image\nimport numpy as np\nimport numpy.typing as ntp\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray\nfrom skimage.feature import hog, local_binary_pattern\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom tqdm.notebook import tqdm\n\ndef extract_features(image: ntp.NDArray[np.float_]) -> ntp.NDArray[np.float_]:\n    return np.hstack([\n        hog(image, orientations=8, pixels_per_cell=(4, 4), cells_per_block=(4, 4), block_norm='L2-Hys', channel_axis=2),\n#         local_binary_pattern(rgb2gray(image), 3 * 8, 3, method='uniform').flatten()\n    ])\n\ndef load_image(\n    filepath: Path,\n    dimensions: tp.Optional[tp.Tuple[int, int]] = None,\n    feature_vector: bool = True\n) -> ntp.NDArray[np.int_]:\n    image = imread(filepath)\n\n    if dimensions is not None:\n        width, height = dimensions\n\n        width = min(width, image.shape[0]) if width is not None else image.shape[0]\n        height = min(height, image.shape[1]) if width is not None else image.shape[1]\n\n        if width != image.shape[0] or height != image.shape[1]:\n            image = resize(image, (width, height))\n\n    if feature_vector:\n        return extract_features(image)\n    else:\n        return image.flatten()\n\ndef load_data(\n    dataset_dir: Path,\n    classes_filepath: Path,\n    limit: tp.Optional[int] = None,\n    **kwargs\n) -> tp.Tuple[ntp.NDArray[np.float_], ntp.NDArray[np.int_]]:\n    df = pd.read_csv(classes_filepath)\n    df.tags = np.char.split(df.tags.values.astype(str))\n\n    encoder = MultiLabelBinarizer().fit(df.tags)\n\n    def load_sample(row: tp.Dict[str, tp.Any]) -> tp.Tuple[ntp.NDArray[np.float_], ntp.NDArray[np.int_]]:\n        filename = f'{row[\"image_name\"]}.jpg'\n\n        image = load_image(dataset_dir / filename, **kwargs)\n        image = image.reshape(1, *image.shape)\n        \n        labels = encoder.transform([row['tags']])\n        \n        return image, labels\n\n    rows = list(df.iterrows())\n    if limit is not None:\n        rows = rows[:limit]\n\n    results = joblib.Parallel(n_jobs=8)(joblib.delayed(load_sample)(row) for _, row in tqdm(rows))\n    X, y = list(zip(*results))\n\n    return np.vstack(X), np.vstack(y), len(encoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T11:05:35.251645Z","iopub.execute_input":"2023-01-12T11:05:35.252165Z","iopub.status.idle":"2023-01-12T11:05:35.277332Z","shell.execute_reply.started":"2023-01-12T11:05:35.252052Z","shell.execute_reply":"2023-01-12T11:05:35.275242Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"X, y, n_classes = load_data(TRAIN_SAMPLES_DIR, TRAIN_LABELS_FILE, limit=10_000, dimensions=(64, 64), feature_vector=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T11:05:35.279763Z","iopub.execute_input":"2023-01-12T11:05:35.280367Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffbdb69b16d4caabfb6f3d660ae3bfd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Achieving reproducibility in our results, requires initializing (also known as `seeding`) the random number generators (RNG) utilized by our dependencies. In order to do so, we designate a `RANDOM_SEED` number, namely `1234`, and we use it to initialize the following RNGs:\n\n- `numpy` (`np.random.seed`)\n- `random` (`random.seed`)\n- `torch (CPU)` (`torch.manual_seed`)\n- `torch (GPU)` (`torch.cuda.manual_seed`)\n\nThe aforementioned RNGs are utilized by `torch`, `numpy` as well as `sklearn` in order to generate random numbers. `random.seed` corresponds to the python standard library RNG. We are seeding each and every one of them in order to cover any possible edge cases, wherein third party code utilizes any of them unbeknownst to us. Lastly, `PYTHONHASHSEED` controls the hashing of str, bytes and datetime objects. More specifically (as stated in the official `Python` documentation):\n\n_\"If this variable is not set or set to random, a random value is used to seed the hashes of str, bytes and datetime objects...\"_","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport random\n\nRANDOM_SEED = 1234\n\nif RANDOM_SEED is not None:\n    np.random.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    torch.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed(RANDOM_SEED)\n    os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(np.arange(X.shape[0]), test_size=0.2, shuffle=True)\n\nX_train, y_train = X[train_indices], y[train_indices]\nX_val, y_val = X[val_indices], y[val_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training set: {X_train.shape}, Validation set: {X_val.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=1024).fit(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ngo.Figure(data=go.Scatter(\n    x=np.arange(1, len(pca.explained_variance_ratio_)),\n    y=np.cumsum(pca.explained_variance_ratio_),\n    fill='tonexty',\n)).update_layout(\n    title=\"Principal Component Analysis\",\n    xaxis_title=\"Number of Components\",\n    yaxis_title=\"Explained Variance\"\n).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multioutput import MultiOutputClassifier\n\ndef create_pipeline(\n    clf: BaseEstimator,\n    n_components: tp.Optional[int] = 1024\n) -> Pipeline:\n    if n_components is not None:\n        return Pipeline([\n            ('transformer', PCA(n_components=n_components)),\n            ('scaler', StandardScaler()),\n            ('model', clf)\n        ])\n\n    return Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', clf)\n    ])\n\ndef create_multioutput_classifier(clf: BaseEstimator, n_components: tp.Optional[int] = 128) -> Pipeline:\n    return create_pipeline(MultiOutputClassifier(estimator=clf), n_components=n_components)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier, RidgeClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nclfs = {\n    'Gradient Boosting': create_multioutput_classifier(HistGradientBoostingClassifier()),\n    'SGD': create_multioutput_classifier(SGDClassifier(max_iter=1000, tol=1e-3)),\n    'SVM': create_multioutput_classifier(SVC()),\n    'Random Forest': create_pipeline(RandomForestClassifier()),\n    'K-NN': create_pipeline(KNeighborsClassifier()),\n    'Ridge Regression': create_pipeline(RidgeClassifierCV()),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nfrom functools import partial\n\ndef get_scorers() -> tp.List[tp.Tuple[str, tp.Callable[[ntp.NDArray[np.int_], ntp.NDArray[np.int_]], np.float_]]]:\n    return [\n        ('F1 (micro)', partial(f1_score, average='micro', zero_division=0)),\n        ('F1 (macro)', partial(f1_score, average='macro', zero_division=0)),\n        ('Accuracy', accuracy_score),\n        ('Precision', partial(precision_score, average='macro', zero_division=0)),\n        ('Recall', partial(recall_score, average='macro', zero_division=0)),\n    ]\n\ndef evaluate(clfs: tp.Tuple[Pipeline,...]) -> pd.DataFrame:\n    scorers = get_scorers()\n\n    progress_bar, scores = tqdm(list(clfs.items())), {}\n    for name, clf in progress_bar:\n        progress_bar.set_description(f'Fitting {name} classifier...')\n        clf.fit(X_train, y_train)\n\n        progress_bar.set_description(f'Evaluating {name} classifier...')\n        y_pred = clf.predict(X_val)\n\n        score_values = []\n        for _, scorer in scorers:\n            score_values.append(scorer(y_val, y_pred))\n        \n        scores[name] = score_values\n\n    return pd.DataFrame.from_dict(scores, columns=[name for name, _ in scorers], orient='index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(clfs).to_csv(BASE_DIR / 'metrics.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}