{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Exploration","metadata":{"id":"8AvFrxTUlVoV"}},{"cell_type":"markdown","source":"## Installing dependencies","metadata":{}},{"cell_type":"code","source":"DEPENDENCIES = [\n    'tf-slim==1.1.0',\n    'numpy==1.21.6',\n    'pandas==1.3.5',\n    'seaborn',\n    'torch==1.11.0+cpu',\n    'torchvision==0.12.0+cpu',\n    'matplotlib==3.5.3',\n    'opencv-python==4.5.4.60',\n    'sklearn==0.0.post1',\n    'skorch==0.12.1',\n    'tqdm',\n    'requests',\n    'plotly==5.11.0',\n    'scikit-image==0.19.3',\n]","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:22:04.686996Z","iopub.execute_input":"2023-02-25T13:22:04.688057Z","iopub.status.idle":"2023-02-25T13:22:04.713380Z","shell.execute_reply.started":"2023-02-25T13:22:04.687944Z","shell.execute_reply":"2023-02-25T13:22:04.712279Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import subprocess\nimport typing as tp\nimport re\n\ndef install_dependencies(dependencies: tp.List[str], show_progress: bool = True) -> tp.Tuple[tp.List[str], tp.List[Exception]]:\n    emit = print if show_progress else lambda x: None\n\n    resolved_dependencies, errors = [], []\n    for dependency in dependencies:\n        emit(f'Installing \"{dependency}\"...')\n\n        try:\n            subprocess.run([\"pip\", \"install\", \"--root-user-action=ignore\", dependency], stdout=subprocess.DEVNULL)\n            \n            if '==' in dependency:\n                dependency = re.search('(.+)==.+', dependency).group(1)\n\n            if '@' in dependency:\n                dependency = re.search('(.+) @ .+', dependency).group(1)\n            \n            pip_freeze = subprocess.Popen((\"pip\", \"freeze\"), stdout=subprocess.PIPE)\n            output = subprocess.check_output((\"grep\", \"-E\", f\"^({dependency}==)|({dependency} @).+$\"), stdin=pip_freeze.stdout)\n            resolved_dependencies.append(output.decode().strip())\n        except subprocess.CalledProcessError as e:\n            errors.append(e)\n    \n    return resolved_dependencies, errors","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:22:04.715521Z","iopub.execute_input":"2023-02-25T13:22:04.715898Z","iopub.status.idle":"2023-02-25T13:22:04.725376Z","shell.execute_reply.started":"2023-02-25T13:22:04.715856Z","shell.execute_reply":"2023-02-25T13:22:04.724224Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\nif (Path(\"/\") / \"kaggle\").is_dir():\n    # Running in kaggle\n    install_dependencies(DEPENDENCIES)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:22:04.726427Z","iopub.execute_input":"2023-02-25T13:22:04.726704Z","iopub.status.idle":"2023-02-25T13:24:29.575105Z","shell.execute_reply.started":"2023-02-25T13:22:04.726681Z","shell.execute_reply":"2023-02-25T13:24:29.572755Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Installing \"tf-slim==1.1.0\"...\nInstalling \"numpy==1.21.6\"...\nInstalling \"pandas==1.3.5\"...\nInstalling \"seaborn\"...\nInstalling \"torch==1.11.0+cpu\"...\nInstalling \"torchvision==0.12.0+cpu\"...\nInstalling \"matplotlib==3.5.3\"...\nInstalling \"opencv-python==4.5.4.60\"...\nInstalling \"sklearn==0.0.post1\"...\nInstalling \"skorch==0.12.1\"...\nInstalling \"tqdm\"...\nInstalling \"requests\"...\nInstalling \"plotly==5.11.0\"...\nInstalling \"scikit-image==0.19.3\"...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nBASE_DIR = Path.cwd()\n\nINPUT_DIR = Path(\"/\") / \"kaggle\" / \"input\"\nif not INPUT_DIR.is_dir():\n    # Not running in Kaggle\n    INPUT_DIR = BASE_DIR / 'data'\n\nDATA_DIR = INPUT_DIR / \"planets-dataset\" / \"planet\" / \"planet\" # https://www.kaggle.com/datasets/nikitarom/planets-dataset\n\nTRAIN_SAMPLES_DIR = DATA_DIR / 'train-jpg'\nTRAIN_LABELS_FILE = DATA_DIR / 'train_classes.csv'\n\nTEST_SAMPLES_DIR = DATA_DIR / 'test-jpg'\nTEST_SAMPLES_DIR_ADDITIONAL = INPUT_DIR / \"planets-dataset\" / 'test-jpg-additional'\nTEST_LABELS_FILE = DATA_DIR / 'sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:29.577373Z","iopub.execute_input":"2023-02-25T13:24:29.577697Z","iopub.status.idle":"2023-02-25T13:24:29.584516Z","shell.execute_reply.started":"2023-02-25T13:24:29.577669Z","shell.execute_reply":"2023-02-25T13:24:29.583289Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport numpy.typing as ntp\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage.feature import hog\nimport pandas as pd\n\nclass Dataset(object):\n    def __init__(\n        self,\n        filepaths: tp.List[Path],\n        labels: tp.List[ntp.NDArray[np.int_]],\n        dimensions: tp.Optional[tp.Tuple[int, int]] = None,\n        feature_vector: bool = True\n    ):\n        self.filepaths = filepaths\n        self.labels = labels\n        \n        self.resize = lambda img: img\n        if dimensions is not None:\n            self.dimensions = dimensions\n            self.resize = lambda img: resize(img, dimensions)\n\n        self.feature_vector = feature_vector\n    \n    @classmethod\n    def _extract_features(cls, image: ntp.NDArray[np.float_]) -> ntp.NDArray[np.float_]:\n        return hog(image, orientations=8, pixels_per_cell=(4, 4), cells_per_block=(4, 4), block_norm='L2-Hys', channel_axis=2)\n\n    def _load_image(self, filepath: Path) -> ntp.NDArray[np.float_]:\n        image = imread(filepath)\n        image = self.resize(image)\n\n        return image\n        \n    def __len__(self) -> int:\n        return len(self.filepaths)\n    \n    def __getitem__(self, index) -> tp.List[tp.Tuple[ntp.NDArray[np.float_], ntp.NDArray[np.int_]]]:\n        filepath, labels = self.filepaths[index], self.labels[index]\n\n        sample = self._load_image(filepath)\n        \n        if self.feature_vector:\n            sample = self._extract_features(sample)\n        else:\n            sample = sample.flatten()\n\n        return sample, labels","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:29.586348Z","iopub.execute_input":"2023-02-25T13:24:29.586792Z","iopub.status.idle":"2023-02-25T13:24:30.505018Z","shell.execute_reply.started":"2023-02-25T13:24:29.586763Z","shell.execute_reply":"2023-02-25T13:24:30.504044Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\nimport joblib\n\nclass DataLoader:\n    def __init__(self, dataset, batch_size: int = 64, shuffle: bool = True, n_workers: int = -1, prefetch_factor: int = 2, collate_fn: tp.Optional[tp.Callable[[tp.Any], tp.Any]] = None, pin_memory: bool = False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.n_workers = n_workers\n        if self.n_workers < 0:\n            self.n_workers = multiprocessing.cpu_count()\n        \n        self.prefetch_factor = prefetch_factor\n        self.collate_fn = collate_fn\n        \n        self.pin_memory = pin_memory\n        \n        self._indices = np.arange(0, len(self.dataset))\n        if self.pin_memory:\n            self._samples = self._load_batch(self._indices)\n\n    def _load_batch(self, indices):\n        if hasattr(self, '_samples'):\n            return [self._samples[index] for index in indices]\n        \n        def wrapper(index):\n            return self.dataset[index]\n        \n        if self.n_workers == 0:\n            return [self.dataset[index] for index in indices]\n\n        return joblib.Parallel(n_jobs=self.n_workers)(joblib.delayed(wrapper)(index) for index in indices)\n        \n    def __len__(self):\n        return max(round(len(self.dataset) / self.batch_size), 1)\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self._indices)\n\n        for prefetched_indices in (self._indices[i:i + self.prefetch_factor * self.batch_size] for i in range(0, len(self._indices), self.batch_size * self.prefetch_factor)):\n            for indices in (prefetched_indices[i:i + self.batch_size] for i in range(0, len(prefetched_indices), self.batch_size)):\n                results = self._load_batch(indices)\n\n                if self.collate_fn is not None:\n                    results = self.collate_fn(results)\n                \n                yield results","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.506236Z","iopub.execute_input":"2023-02-25T13:24:30.506480Z","iopub.status.idle":"2023-02-25T13:24:30.537103Z","shell.execute_reply.started":"2023-02-25T13:24:30.506457Z","shell.execute_reply":"2023-02-25T13:24:30.536290Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef create_train_val_split(dataset_dir, classes_filepath, dimensions=None, feature_vector=False, val_ratio=0.2, batch_size=64, n_workers=-1, pin_memory=False, return_arrays=False, limit=None):\n    def collate_fn(results):\n        X, y = list(zip(*results))\n\n        return np.vstack(X), np.vstack(y)\n    \n    df = pd.read_csv(classes_filepath)\n    if limit is not None:\n        df = df.sample(n=limit, replace=False)\n\n    df.tags = np.char.split(df.tags.values.astype(str))\n\n    train_indices, val_indices = train_test_split(np.arange(len(df)), test_size=val_ratio, shuffle=True)\n\n    df_train, df_val = df.iloc[train_indices], df.iloc[val_indices]\n    \n    image_names_train, image_names_val = df_train['image_name'].values.tolist(), df_val['image_name'].values.tolist()\n    filepaths_train = [dataset_dir / f'{image_name}.jpg' for image_name in image_names_train]\n    filepaths_val = [dataset_dir / f'{image_name}.jpg' for image_name in image_names_val]\n    \n    encoder = MultiLabelBinarizer()\n    labels_train = encoder.fit_transform(df_train.tags)\n    labels_val = encoder.transform(df_val.tags)\n\n    dataset_train = Dataset(filepaths_train, labels_train, dimensions=dimensions, feature_vector=feature_vector)\n    dataset_val = Dataset(filepaths_val, labels_val, dimensions=dimensions, feature_vector=feature_vector)\n    \n    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, n_workers=n_workers, prefetch_factor=10, collate_fn=collate_fn, pin_memory=pin_memory)\n    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, n_workers=n_workers, prefetch_factor=10, collate_fn=collate_fn, pin_memory=pin_memory)\n    \n    if return_arrays:\n        X_train, y_train = [], []\n        for batch_X, batch_y in dataloader_train:\n            X_train.extend(batch_X), y_train.extend(batch_y)\n\n        X_train, y_train = np.vstack(X_train), np.vstack(y_train)\n        \n        X_val, y_val = [], []\n        for batch_X, batch_y in dataloader_val:\n            X_val.extend(batch_X), y_val.extend(batch_y)\n\n        X_val, y_val = np.vstack(X_val), np.vstack(y_val)\n        \n        return encoder, X_train, y_train, X_val, y_val\n    \n    return encoder, dataloader_train, dataloader_val","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.538201Z","iopub.execute_input":"2023-02-25T13:24:30.538469Z","iopub.status.idle":"2023-02-25T13:24:30.629326Z","shell.execute_reply.started":"2023-02-25T13:24:30.538443Z","shell.execute_reply":"2023-02-25T13:24:30.628672Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Structuring our models","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom enum import IntEnum, auto\n\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\n\nclass OnlinePipeline(Pipeline):\n    def partial_fit(self, X, y=None, classes=None):\n        for i, step in enumerate(self.steps):\n            name, est = step\n            if i < len(self.steps) - 1:\n                est.partial_fit(X, y)\n                X = est.transform(X)\n            else:\n                est.partial_fit(X, y, classes=classes)\n\n        return self\n\nclass Reduction(IntEnum):\n    SLICE = auto()\n    DICTIONARY_LEARNING = auto()\n    PCA = auto()\n    KMEANS = auto()\n    \ndef create_multioutput_classifier(clf: BaseEstimator, reduction: tp.Optional[Reduction] = None, n_components: tp.Optional[int] = 1024) -> Pipeline:\n    if reduction is not None:\n        assert n_components is not None, \"'n_components' has not been specified\"\n\n    transformer = None\n    if reduction is not None:\n        if reduction == Reduction.DICTIONARY_LEARNING:\n            transformer = MiniBatchDictionaryLearning(n_components=n_components)\n        elif reduction == Reduction.PCA:\n            transformer = IncrementalPCA(n_components=n_components)\n        elif reduction == Reduction.KMEANS:\n            transformer = MiniBatchKMeans(n_clusters=n_components)\n        else:\n            raise ValueError(f'{reduction} is not supported')\n\n    if transformer is not None:\n        return OnlinePipeline([('scaler', StandardScaler()), ('transformer', transformer), ('model', MultiOutputClassifier(estimator=clf))])\n    \n    return OnlinePipeline([('scaler', StandardScaler()), ('model', MultiOutputClassifier(estimator=clf))])","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.630232Z","iopub.execute_input":"2023-02-25T13:24:30.631280Z","iopub.status.idle":"2023-02-25T13:24:30.817264Z","shell.execute_reply.started":"2023-02-25T13:24:30.631254Z","shell.execute_reply":"2023-02-25T13:24:30.816356Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating our models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron\n\ndef get_clfs(**kwargs):\n    return {\n        'SGDClassifier': create_multioutput_classifier(SGDClassifier(max_iter=1000, tol=1e-3), **kwargs),\n        'PassiveAggressiveClassifier': create_multioutput_classifier(PassiveAggressiveClassifier(max_iter=1000, tol=1e-3), **kwargs),\n    }","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.818746Z","iopub.execute_input":"2023-02-25T13:24:30.819494Z","iopub.status.idle":"2023-02-25T13:24:30.826411Z","shell.execute_reply.started":"2023-02-25T13:24:30.819441Z","shell.execute_reply":"2023-02-25T13:24:30.825800Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, fbeta_score\nfrom functools import partial\nfrom tqdm.notebook import tqdm\n\ndef get_scorers() -> tp.List[tp.Tuple[str, tp.Callable[[ntp.NDArray[np.int_], ntp.NDArray[np.int_]], np.float_]]]:\n    return [\n        ('F1 (micro)', partial(f1_score, average='micro', zero_division=0)),\n        ('F1 (macro)', partial(f1_score, average='macro', zero_division=0)),\n        ('F1 (samples)', partial(f1_score, average='samples', zero_division=0)),\n        ('F2 (micro)', partial(fbeta_score, beta=2, average='micro', zero_division=0)),\n        ('F2 (macro)', partial(fbeta_score, beta=2, average='macro', zero_division=0)),\n        ('F2 (samples)', partial(fbeta_score, beta=2, average='samples', zero_division=0)),\n        ('Accuracy', accuracy_score),\n        ('Precision', partial(precision_score, average='macro', zero_division=0)),\n        ('Recall', partial(recall_score, average='macro', zero_division=0)),\n    ]\n\ndef evaluate(\n    clf: Pipeline, name: str,\n    X_train: ntp.NDArray[np.float_], y_train: ntp.NDArray[np.int_],\n    X_val: ntp.NDArray[np.float_], y_val: ntp.NDArray[np.int_]\n) -> pd.DataFrame:\n    scorers = get_scorers()\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_val)\n\n    scores = []\n    for _, scorer in scorers:\n        scores.append(scorer(y_val, y_pred))\n\n    return pd.DataFrame.from_dict({ name: scores }, columns=[name for name, _ in scorers], orient='index')","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.829481Z","iopub.execute_input":"2023-02-25T13:24:30.830322Z","iopub.status.idle":"2023-02-25T13:24:30.931746Z","shell.execute_reply.started":"2023-02-25T13:24:30.830290Z","shell.execute_reply":"2023-02-25T13:24:30.930849Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def collect_results(\n    X_train: ntp.NDArray[np.float_], y_train: ntp.NDArray[np.int_],\n    X_val: ntp.NDArray[np.float_], y_val: ntp.NDArray[np.int_],\n    dimensions=None, feature_vector=False, limit=None, **kwargs\n):\n    clfs = get_clfs(**kwargs)\n    \n    df, progress_bar, scores = None, tqdm(clfs.items(), position=0), {}\n    for name, clf in progress_bar:\n        progress_bar.set_description(name)\n        \n        try:\n            results = evaluate(clf, name, X_train, y_train, X_val, y_val)\n\n            if df is None:\n                df = results\n            else:\n                df = pd.concat((df, results))\n        except Exception as e:\n            print(f'Unexpected error: {e}')\n    \n    identifiers = dict(\n        dimensions=f'{dimensions[0]}_{dimensions[0]}' if dimensions is not None else 'None',\n        feature_vector=feature_vector,\n        limit=limit\n    )\n    \n    suffixes = '_'.join(f'{key}_{value}' for key, value in { **identifiers, **kwargs }.items()).lower()\n    filename = f'metrics_{suffixes}.csv'\n    df.to_csv(BASE_DIR / filename, index=False)\n    \n    return clfs, df","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.933110Z","iopub.execute_input":"2023-02-25T13:24:30.933430Z","iopub.status.idle":"2023-02-25T13:24:30.941544Z","shell.execute_reply.started":"2023-02-25T13:24:30.933405Z","shell.execute_reply":"2023-02-25T13:24:30.940930Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Achieving reproducibility in our results, requires initializing (also known as `seeding`) the random number generators (RNG) utilized by our dependencies. In order to do so, we designate a `RANDOM_SEED` number, namely `1234`, and we use it to initialize the following RNGs:\n\n- `numpy` (`np.random.seed`)\n- `random` (`random.seed`)\n- `torch (CPU)` (`torch.manual_seed`)\n- `torch (GPU)` (`torch.cuda.manual_seed`)\n\nThe aforementioned RNGs are utilized by `torch`, `numpy` as well as `sklearn` in order to generate random numbers. `random.seed` corresponds to the python standard library RNG. We are seeding each and every one of them in order to cover any possible edge cases, wherein third party code utilizes any of them unbeknownst to us. Lastly, `PYTHONHASHSEED` controls the hashing of str, bytes and datetime objects. More specifically (as stated in the official `Python` documentation):\n\n_\"If this variable is not set or set to random, a random value is used to seed the hashes of str, bytes and datetime objects...\"_","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport random\n\nRANDOM_SEED = 1234\n\nif RANDOM_SEED is not None:\n    np.random.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    torch.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed(RANDOM_SEED)\n    os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:24:30.942553Z","iopub.execute_input":"2023-02-25T13:24:30.943215Z","iopub.status.idle":"2023-02-25T13:24:32.245665Z","shell.execute_reply.started":"2023-02-25T13:24:30.943188Z","shell.execute_reply":"2023-02-25T13:24:32.244567Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"LIMIT = 64 * 1 # Set to None to train in the entire dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:28:52.664959Z","iopub.execute_input":"2023-02-25T13:28:52.665799Z","iopub.status.idle":"2023-02-25T13:28:52.670499Z","shell.execute_reply.started":"2023-02-25T13:28:52.665765Z","shell.execute_reply":"2023-02-25T13:28:52.669092Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"configuration, best_score = {}, -float('inf')\nfor feature_vector in tqdm([False, True], position=-2, leave=False):\n    _, X_train, y_train, X_val, y_val = create_train_val_split(\n        TRAIN_SAMPLES_DIR, TRAIN_LABELS_FILE,\n        dimensions=None, feature_vector=feature_vector, limit=LIMIT, pin_memory=True, return_arrays=True\n    )\n\n    for reduction in tqdm([None, Reduction.PCA, Reduction.KMEANS, Reduction.DICTIONARY_LEARNING], position=-1, leave=False):\n        df = collect_results(X_train, y_train, X_val, y_val, dimensions=None, feature_vector=feature_vector, limit=LIMIT, reduction=reduction, n_components=32)[1]\n        \n        score = df['F1 (samples)'].max()\n        if score > best_score:\n            configuration, best_score = dict(feature_vector=feature_vector, reduction=reduction, model=df[df['F1 (samples)'] == df['F1 (samples)'].max()].index[0]), score","metadata":{"execution":{"iopub.status.busy":"2023-02-25T13:29:34.599064Z","iopub.execute_input":"2023-02-25T13:29:34.599681Z","iopub.status.idle":"2023-02-25T14:08:13.717712Z","shell.execute_reply.started":"2023-02-25T13:29:34.599650Z","shell.execute_reply":"2023-02-25T14:08:13.716977Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:876: UserWarning: unknown class(es) ['cloudy', 'haze'] will be ignored\n  \"unknown class(es) {0} will be ignored\".format(sorted(unknown, key=str))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18587705aae144f093c4a03789fb50dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb19c73322e64e96aaa71dcf461f8276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1de0f1052841eca844d84b4314ab41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0c4af811d844c89fa14078ce4898ac"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:876: UserWarning: unknown class(es) ['blooming', 'slash_burn'] will be ignored\n  \"unknown class(es) {0} will be ignored\".format(sorted(unknown, key=str))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd8bb970d4a847838924bd4a2b280629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0877e86b67f7485daa7080356ca95ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb30f60278a94f9cb9d8dbbc8d4837be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c3d6d3acfc464691bce0c5df7f8061"}},"metadata":{}}]}]}