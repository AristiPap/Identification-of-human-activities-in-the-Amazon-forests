{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"8AvFrxTUlVoV"},"source":["# Transfer Learning"]},{"cell_type":"markdown","metadata":{},"source":["## Installing dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:32:34.025568Z","iopub.status.busy":"2023-02-15T20:32:34.025171Z","iopub.status.idle":"2023-02-15T20:32:34.032117Z","shell.execute_reply":"2023-02-15T20:32:34.030689Z","shell.execute_reply.started":"2023-02-15T20:32:34.025533Z"},"trusted":true},"outputs":[],"source":["DEPENDENCIES = [\n","    'tf-slim==1.1.0',\n","    'numpy==1.21.6',\n","    'pandas==1.3.5',\n","    'seaborn',\n","    'torch==1.11.0',\n","    'torchvision==0.12.0',\n","    'matplotlib==3.5.3',\n","    'opencv-python==4.5.4.60',\n","    'sklearn==0.0.post1',\n","    'skorch==0.12.1',\n","    'tqdm',\n","    'requests',\n","    'plotly==5.11.0',\n","    'scikit-image==0.19.3',\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:32:34.034705Z","iopub.status.busy":"2023-02-15T20:32:34.034200Z","iopub.status.idle":"2023-02-15T20:32:34.047428Z","shell.execute_reply":"2023-02-15T20:32:34.046504Z","shell.execute_reply.started":"2023-02-15T20:32:34.034668Z"},"trusted":true},"outputs":[],"source":["import subprocess\n","import typing as tp\n","import re\n","\n","def install_dependencies(dependencies: tp.List[str], show_progress: bool = True) -> tp.Tuple[tp.List[str], tp.List[Exception]]:\n","    emit = print if show_progress else lambda x: None\n","\n","    resolved_dependencies, errors = [], []\n","    for dependency in dependencies:\n","        emit(f'Installing \"{dependency}\"...')\n","\n","        try:\n","            subprocess.run([\"pip\", \"install\", \"--root-user-action=ignore\", dependency], stdout=subprocess.DEVNULL)\n","            \n","            if '==' in dependency:\n","                dependency = re.search('(.+)==.+', dependency).group(1)\n","\n","            if '@' in dependency:\n","                dependency = re.search('(.+) @ .+', dependency).group(1)\n","            \n","            pip_freeze = subprocess.Popen((\"pip\", \"freeze\"), stdout=subprocess.PIPE)\n","            output = subprocess.check_output((\"grep\", \"-E\", f\"^({dependency}==)|({dependency} @).+$\"), stdin=pip_freeze.stdout)\n","            resolved_dependencies.append(output.decode().strip())\n","        except subprocess.CalledProcessError as e:\n","            errors.append(e)\n","    \n","    return resolved_dependencies, errors"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:32:34.051440Z","iopub.status.busy":"2023-02-15T20:32:34.050248Z","iopub.status.idle":"2023-02-15T20:35:02.283034Z","shell.execute_reply":"2023-02-15T20:35:02.281866Z","shell.execute_reply.started":"2023-02-15T20:32:34.051403Z"},"trusted":true},"outputs":[],"source":["install_dependencies(DEPENDENCIES)"]},{"cell_type":"markdown","metadata":{},"source":["## Seeding RNGs"]},{"cell_type":"markdown","metadata":{},"source":["Achieving reproducibility in our results, requires initializing (also known as `seeding`) the random number generators (RNG) utilized by our dependencies. In order to do so, we designate a `RANDOM_SEED` number, namely `1234`, and we use it to initialize the following RNGs:\n","\n","- `numpy` (`np.random.seed`)\n","- `random` (`random.seed`)\n","- `torch (CPU)` (`torch.manual_seed`)\n","- `torch (GPU)` (`torch.cuda.manual_seed`)\n","\n","The aforementioned RNGs are utilized by `torch`, `numpy` as well as `sklearn` in order to generate random numbers. `random.seed` corresponds to the python standard library RNG. We are seeding each and every one of them in order to cover any possible edge cases, wherein third party code utilizes any of them unbeknownst to us. Lastly, `PYTHONHASHSEED` controls the hashing of str, bytes and datetime objects. More specifically (as stated in the official `Python` documentation):\n","\n","_\"If this variable is not set or set to random, a random value is used to seed the hashes of str, bytes and datetime objects...\"_"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:02.285686Z","iopub.status.busy":"2023-02-15T20:35:02.285269Z","iopub.status.idle":"2023-02-15T20:35:03.872132Z","shell.execute_reply":"2023-02-15T20:35:03.871090Z","shell.execute_reply.started":"2023-02-15T20:35:02.285640Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import random\n","\n","RANDOM_SEED = 1234\n","\n","if RANDOM_SEED is not None:\n","    np.random.seed(RANDOM_SEED)\n","    random.seed(RANDOM_SEED)\n","    torch.manual_seed(RANDOM_SEED)\n","    torch.cuda.manual_seed(RANDOM_SEED)\n","    os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:03.875070Z","iopub.status.busy":"2023-02-15T20:35:03.874533Z","iopub.status.idle":"2023-02-15T20:35:03.881855Z","shell.execute_reply":"2023-02-15T20:35:03.880926Z","shell.execute_reply.started":"2023-02-15T20:35:03.875040Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","BASE_DIR = Path.cwd()\n","INPUT_DIR = Path(\"/\") / \"kaggle\" / \"input\"\n","DATA_DIR = INPUT_DIR / \"planets-dataset\" / \"planet\" / \"planet\" # https://www.kaggle.com/datasets/nikitarom/planets-dataset\n","\n","TRAIN_SAMPLES_DIR = DATA_DIR / 'train-jpg'\n","TRAIN_LABELS_FILE = DATA_DIR / 'train_classes.csv'\n","\n","TEST_SAMPLES_DIR = DATA_DIR / 'test-jpg'\n","TEST_LABELS_FILE = DATA_DIR / 'sample_submission.csv'\n","\n","MODEL_WEIGHTS_DIR = INPUT_DIR / 'resnet-weights'"]},{"cell_type":"markdown","metadata":{},"source":["We define our custom Dataset class to manipulate batches of data between RAM and Disk more easily. Some point of attentions:\n","\n","- __init__: we pass the dataframe along with the target, the transformation, the file path and is_train flag. It is important to distinguish the training phase from the testing phase because we use test augmentation. Test augmentation (TTA) is helpful to diversify our training dataset and build a more robust model. It is applied on each image for each batch, meaning that is doesn't increase the length of our training dataset per say, but it transforms each image randomly during execution time.\n","- __getitem__: we define what the dataset return upon iteration. It needs to load both image and target. collate_fn: we use this function within the following DataLoader instance. It corresponds to the batch manipulation. This is were transform is called. We also proceed to train and test augmentation there.\n","- collate_fn: we use this function within the following DataLoader instance. It corresponds to the batch manipulation. This is were transform is called. We also proceed to train and test augmentation there."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:03.884004Z","iopub.status.busy":"2023-02-15T20:35:03.883384Z","iopub.status.idle":"2023-02-15T20:35:04.048608Z","shell.execute_reply":"2023-02-15T20:35:04.047582Z","shell.execute_reply.started":"2023-02-15T20:35:03.883967Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import cv2\n","import torch\n","import numpy.typing as ntp\n","\n","Transform = tp.Callable[[torch.Tensor], torch.Tensor]\n","\n","class AmazonDataset(Dataset):\n","    def __init__(self, dataset_dir: Path, image_names: tp.List[str], tags: tp.List[tp.List[int]], transform: tp.Optional[Transform] = None) -> None:\n","        super().__init__()\n","\n","        self.dataset_dir = dataset_dir\n","        self.image_names = image_names\n","        self.tags = tags\n","        self.transform = transform\n","\n","    def __len__(self) -> int:\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx: int) -> tp.Tuple[ntp.NDArray[np.float_], ntp.NDArray[np.int_]]:\n","        image = self.load_image(idx)\n","        tags = self.load_tags(idx)\n","        \n","        return image, tags\n","\n","    def load_tags(self, idx: int) -> torch.Tensor:\n","        tags = self.tags[idx]\n","        tags = torch.as_tensor(tags)\n","        tags = tags.float()\n","        \n","        return tags\n","    \n","    def load_image(self, idx: int) -> torch.Tensor:\n","        image_name = self.image_names[idx]\n","        filename = f'{image_name}.jpg'\n","        filepath = self.dataset_dir / filename\n","\n","        image = cv2.imread(str(filepath))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = torch.tensor(image)\n","        image = image.permute(2, 0, 1)\n","        image = self.transform(image)\n","        image = image.float()\n","        \n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.050470Z","iopub.status.busy":"2023-02-15T20:35:04.050074Z","iopub.status.idle":"2023-02-15T20:35:04.249971Z","shell.execute_reply":"2023-02-15T20:35:04.248941Z","shell.execute_reply.started":"2023-02-15T20:35:04.050431Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as T\n","\n","transform_train = T.Compose([\n","    T.ToPILImage(),\n","    T.Resize(224),\n","    T.ToTensor(),\n","    T.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225],\n","    ),\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(180)\n","])\n","\n","transform_val = T.Compose([\n","    T.ToPILImage(),\n","    T.Resize(224),\n","    T.ToTensor(),\n","    T.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225],\n","    )\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.251849Z","iopub.status.busy":"2023-02-15T20:35:04.251503Z","iopub.status.idle":"2023-02-15T20:35:04.273120Z","shell.execute_reply":"2023-02-15T20:35:04.272265Z","shell.execute_reply.started":"2023-02-15T20:35:04.251813Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from torch.utils.data import DataLoader\n","\n","\n","def create_datasets(\n","    dataset_dir: Path,\n","    classes_filepath: Path,\n","    batch_size: int = 64,\n","    test_size: float = 0.2,\n","    shuffle: bool = False,\n","    limit: tp.Optional[int] = None\n",") -> tp.Tuple[DataLoader, DataLoader, MultiLabelBinarizer]:\n","    df = pd.read_csv(classes_filepath)\n","    \n","    if limit is not None:\n","        df = df.head(limit)\n","    \n","    df.tags = np.char.split(df.tags.values.astype(str))\n","    \n","    df_train, df_val = train_test_split(df, test_size=test_size, shuffle=shuffle)\n","\n","    encoder = MultiLabelBinarizer()\n","    tags_train = encoder.fit_transform(df_train.tags)\n","    tags_val = encoder.transform(df_val.tags)\n","    \n","    dataset_train = AmazonDataset(dataset_dir, df_train.image_name.to_numpy(), tags_train, transform_train)\n","    dataset_val = AmazonDataset(dataset_dir, df_val.image_name.to_numpy(), tags_val, transform_val)\n","\n","    dataloader_train = DataLoader(\n","      dataset_train,\n","      batch_size=batch_size,\n","      shuffle=True,\n","    )\n","\n","    dataloader_val = DataLoader(\n","      dataset_val,\n","      batch_size=batch_size,\n","      shuffle=True,\n","    )\n","\n","    return dataloader_train, dataloader_val, encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.276292Z","iopub.status.busy":"2023-02-15T20:35:04.276019Z","iopub.status.idle":"2023-02-15T20:35:04.448943Z","shell.execute_reply":"2023-02-15T20:35:04.447907Z","shell.execute_reply.started":"2023-02-15T20:35:04.276266Z"},"trusted":true},"outputs":[],"source":["dataloader_train, dataloader_val, encoder = create_datasets(TRAIN_SAMPLES_DIR, TRAIN_LABELS_FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.451120Z","iopub.status.busy":"2023-02-15T20:35:04.450576Z","iopub.status.idle":"2023-02-15T20:35:04.457267Z","shell.execute_reply":"2023-02-15T20:35:04.456146Z","shell.execute_reply.started":"2023-02-15T20:35:04.451081Z"},"trusted":true},"outputs":[],"source":["print(f'Training set: {len(dataloader_train)}, Validation set: {len(dataloader_val)}')"]},{"cell_type":"markdown","metadata":{},"source":["For optimal performances, resnet18 need input shape that are multiple of 32 and in our case we have input of size 256. From 256, the closest multiple of 32 is 224.\n","\n","Therefore, we rescale our input data using this multiple, and we also normalize our dataset based on resnet pretrained mean and standard deviation intensity values. ToTensor() is useful to normalize our image values from 0-255 range to 0-1 range."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.462936Z","iopub.status.busy":"2023-02-15T20:35:04.462448Z","iopub.status.idle":"2023-02-15T20:35:04.476274Z","shell.execute_reply":"2023-02-15T20:35:04.474942Z","shell.execute_reply.started":"2023-02-15T20:35:04.462898Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","from torchvision import models\n","\n","class ResNet(nn.Module):\n","    def __init__(self, freeze: bool = True, dropout: float = 0.2):\n","        super().__init__()\n","\n","        self.resnet18 = models.resnet18(pretrained=True)\n","        for parameter in self.resnet18.parameters():\n","            parameter.require_grad = not freeze\n","        \n","        self.resnet18.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","        self.resnet18.fc = nn.Sequential(\n","          nn.Flatten(),\n","          nn.Linear(512, 128), # 512 for resnet18 or 2048 for resnet 50\n","          nn.ReLU(inplace=True),\n","          nn.Dropout(dropout),\n","          nn.Linear(128, 17)\n","        )\n","\n","    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n","        return self.resnet18(batch)\n","\n","    @classmethod\n","    def from_device(cls, *args, device_id=\"cpu\", **kwargs) -> tp.Tuple[\"ResNet\", torch.device]:\n","        # We firstly initialize an instance of our model\n","        model = cls(*args, **kwargs)\n","\n","        # If the cuda backend is available then change the device type to GPU\n","        if torch.cuda.is_available():\n","            device_id = \"cuda:0\"\n","            # Given that there are multiple GPUs available wrap the model\n","            # in `nn.DataParallel` in order to take advantage of them\n","            if torch.cuda.device_count() > 1:\n","                model = nn.DataParallel(model)\n","\n","        # Retrieve the `torch.device` corresponding to `device_id`\n","        # and transfer the model to it\n","        device = torch.device(device_id)\n","\n","        return model.to(device), device\n","\n","    @classmethod\n","    def from_file(cls, filename: Path, *args, device_id: str = \"cpu\", **kwargs) -> tp.Tuple[\"ResNet\", torch.device]:\n","        # Firstly initialize the model and retrieve the device wherein it is located\n","        model, device = cls.from_device(*args, device_id=device_id, **kwargs)\n","\n","        # Load the model state from the supplied file\n","        # and dynamically remap it to the device at hand using the `map_location`\n","        model.load_state_dict(torch.load(filename, map_location=device))\n","\n","        return model, device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.478679Z","iopub.status.busy":"2023-02-15T20:35:04.478330Z","iopub.status.idle":"2023-02-15T20:35:04.490228Z","shell.execute_reply":"2023-02-15T20:35:04.489204Z","shell.execute_reply.started":"2023-02-15T20:35:04.478645Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","\n","class EarlyStoppingStrategy(object):\n","    def __init__(self, \n","        tolerance: int = 5,\n","        min_delta: float = 0,\n","        checkpoint_dir: tp.Optional[Path] = None,\n","    ):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.checkpoint_dir = checkpoint_dir\n","\n","        self.best_validation_loss = float('inf')\n","        self.counter = 0\n","\n","    def __call__(self, validation_loss, model):\n","        best_validation_loss = self.best_validation_loss\n","        self.best_validation_loss = min(self.best_validation_loss, validation_loss)\n","\n","        if best_validation_loss - validation_loss < self.min_delta:\n","            # if validation loss value at hand is not at least `min_delta`\n","            # smaller than the so far smallest validation loss\n","            # increment the tolerance counter by 1\n","            # If the counter exceeds the specified tolerance level we should\n","            # halt the training procedure\n","            self.counter += 1\n","            if self.counter > self.tolerance:\n","                return True\n","        else:\n","            # Otherwise (meaning the validation loss has decreased considerably)\n","            # reset the tolerance counter and persist the model state\n","            self.counter = 0\n","\n","            if self.checkpoint_dir is not None:\n","                filename = f'{model.__class__.__name__}_{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S_%f\")}.pkl'\n","\n","                torch.save(model.state_dict(), self.checkpoint_dir / filename)\n","\n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.492444Z","iopub.status.busy":"2023-02-15T20:35:04.491679Z","iopub.status.idle":"2023-02-15T20:35:04.509080Z","shell.execute_reply":"2023-02-15T20:35:04.508035Z","shell.execute_reply.started":"2023-02-15T20:35:04.492403Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","from torch import nn, optim\n","from tqdm.notebook import tqdm\n","\n","def train(\n","    model: nn.Module,\n","    device: torch.device,\n","    train_loader: DataLoader,\n","    validation_loader: DataLoader,\n","    early_stopping_strategy: tp.Optional[EarlyStoppingStrategy] = None,\n","    n_epochs: int = 100,\n","    weight_decay: float = 0.0,\n","    lr=0.001,\n","    eps=1e-08,\n",") -> tp.Tuple[int, ...]:\n","    # Define a method to retrieve the tqdm progress bar postfix data\n","    batch_index, train_losses, validation_losses = 0, [0], [0]\n","    def get_postfix():\n","        return {\n","            'train': f'{train_losses[-1]:.3f}',\n","            'validation': f'{validation_losses[-1]:.3f}',\n","            'batch': f'{batch_index:02d} / {len(train_loader):02d}'\n","        }\n","\n","    if early_stopping_strategy is not None:\n","        def get_postfix():\n","            return {\n","                'train': f'{train_losses[-1]:.3f}',\n","                'validation': f'{validation_losses[-1]:.3f}',\n","                'tolerance': f'{early_stopping_strategy.counter}/{early_stopping_strategy.tolerance}',\n","                'batch': f'{batch_index:02d} / {len(train_loader):02d}'\n","            }\n","\n","    criterion = nn.BCEWithLogitsLoss()\n","    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay, lr=lr, eps=eps)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    epochs_progress_bar = tqdm(range(n_epochs), desc='epochs', position=0)\n","\n","    timestamp = time.time()\n","    for _ in epochs_progress_bar:\n","        # We set the model to training mode\n","        model.train()\n","\n","        train_loss = 0\n","        for batch_index, (batch_X, batch_y) in enumerate(train_loader):\n","            # Transfer the data to the available device backend (CPU/GPU)\n","            batch_X, batch_y, = batch_X.to(device), batch_y.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(batch_X)\n","            loss = criterion(output, batch_y)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            train_loss += loss.item() / len(train_loader)\n","\n","            epochs_progress_bar.set_postfix(**get_postfix())\n","\n","        train_losses.append(train_loss)\n","\n","        epochs_progress_bar.set_postfix(**get_postfix())\n","\n","        scheduler.step()\n","        \n","        # We set the model to evaluation mode\n","        model.eval()\n","        with torch.no_grad():\n","            # Turn of gradient calculation\n","            validation_loss = 0\n","            for batch_X, batch_y in validation_loader:\n","                batch_X, batch_y, = batch_X.to(device), batch_y.to(device)\n","\n","                output = model(batch_X)\n","                loss = criterion(output, batch_y)\n","\n","                validation_loss += loss.item() / len(validation_loader)\n","\n","            validation_losses.append(validation_loss)\n","\n","            epochs_progress_bar.set_postfix(**get_postfix())\n","            \n","        # Invoke the early stopping strategy with the current validation loss\n","        # in order to increment/reset its internal tolerance counter\n","        # and determine whether or not to halt the training process\n","        if early_stopping_strategy is not None:\n","            if early_stopping_strategy(validation_losses[-1], model):\n","                break\n","    \n","    return time.time() - timestamp, np.array(train_losses[1:]), np.array(validation_losses[1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.511958Z","iopub.status.busy":"2023-02-15T20:35:04.511627Z","iopub.status.idle":"2023-02-15T20:35:04.522314Z","shell.execute_reply":"2023-02-15T20:35:04.521361Z","shell.execute_reply.started":"2023-02-15T20:35:04.511917Z"},"trusted":true},"outputs":[],"source":["def get_most_recent_checkpoint(model_weights_directory: Path, prefix: str = 'ResNet', suffix: str = 'pkl') -> ResNet:\n","    most_recent_checkpoint, most_recent_timestamp = None, datetime.min\n","    for file in model_weights_directory.glob(f'**/*.{suffix}'):\n","        filename = file.name\n","        timestamp = datetime.strptime(filename, f\"{prefix}_%d_%m_%Y_%H_%M_%S_%f.{suffix}\")\n","        \n","        if timestamp > most_recent_timestamp:\n","            most_recent_checkpoint, most_recent_timestamp = filename, timestamp\n","    \n","    return model_weights_directory / most_recent_checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.527030Z","iopub.status.busy":"2023-02-15T20:35:04.525513Z","iopub.status.idle":"2023-02-15T20:35:04.541469Z","shell.execute_reply":"2023-02-15T20:35:04.540509Z","shell.execute_reply.started":"2023-02-15T20:35:04.526989Z"},"trusted":true},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","def learning_curves(\n","    train_losses,\n","    validation_losses,\n","    title: str = 'Loss per Epoch',\n","    label_x: str = 'Epochs',\n","    label_y: str = 'Loss',\n",") -> None:\n","    epochs = np.arange(max(len(train_losses), len(validation_losses)))\n","\n","    go.Figure(data=[\n","            go.Scatter(name='Training', x=epochs, y=train_losses, mode='lines'),\n","            go.Scatter(name='Validation', x=epochs, y=validation_losses, mode='lines'),\n","    ]).update_layout(title=title, xaxis_title=label_x, yaxis_title=label_y).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:04.543433Z","iopub.status.busy":"2023-02-15T20:35:04.542989Z","iopub.status.idle":"2023-02-15T20:35:08.506813Z","shell.execute_reply":"2023-02-15T20:35:08.505746Z","shell.execute_reply.started":"2023-02-15T20:35:04.543395Z"},"trusted":true},"outputs":[],"source":["most_recent_checkpoint = get_most_recent_checkpoint(MODEL_WEIGHTS_DIR)\n","\n","if most_recent_checkpoint is None:\n","    checkpoint_dir = Path.cwd() / 'models'\n","    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n","\n","    early_stopping_strategy = EarlyStoppingStrategy(\n","        tolerance=5,\n","        min_delta=0.001,\n","        checkpoint_dir=checkpoint_dir\n","    )\n","\n","    model, device = ResNet.from_device()\n","\n","    _, train_losses, validation_losses = train(\n","        model, device,\n","        dataloader_train, dataloader_val,\n","        early_stopping_strategy=early_stopping_strategy, n_epochs=20, lr=1e-4\n","    )\n","    \n","    learning_curves(train_losses, validation_losses)\n","else:\n","    model, device = ResNet.from_file(most_recent_checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluating our ResNet architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:08.510073Z","iopub.status.busy":"2023-02-15T20:35:08.509289Z","iopub.status.idle":"2023-02-15T20:35:08.517020Z","shell.execute_reply":"2023-02-15T20:35:08.515882Z","shell.execute_reply.started":"2023-02-15T20:35:08.510030Z"},"trusted":true},"outputs":[],"source":["def create_test_dataset(\n","    dataset_dir: Path,\n","    classes_filepath: Path,\n","    encoder: MultiLabelBinarizer,\n","    batch_size: int = 32,\n",") -> DataLoader:\n","    df = pd.read_csv(classes_filepath)\n","    df.tags = np.char.split(df.tags.values.astype(str))\n","\n","    tags = encoder.transform(df.tags)\n","    \n","    dataset = AmazonDataset(dataset_dir, df.image_name.to_numpy(), tags, transform_val)\n","\n","    dataloader = DataLoader(\n","      dataset,\n","      batch_size=batch_size\n","    )\n","\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T20:35:08.518233Z","iopub.status.busy":"2023-02-15T20:35:08.517962Z","iopub.status.idle":"2023-02-15T20:35:08.904463Z","shell.execute_reply":"2023-02-15T20:35:08.903432Z","shell.execute_reply.started":"2023-02-15T20:35:08.518208Z"},"trusted":true},"outputs":[],"source":["dataloader_test = create_test_dataset(TEST_SAMPLES_DIR, TEST_LABELS_FILE, encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T21:29:18.601063Z","iopub.status.busy":"2023-02-15T21:29:18.600380Z","iopub.status.idle":"2023-02-15T21:29:18.612004Z","shell.execute_reply":"2023-02-15T21:29:18.610922Z","shell.execute_reply.started":"2023-02-15T21:29:18.601017Z"},"trusted":true},"outputs":[],"source":["def predict(\n","    model: nn.Module,\n","    device: torch.device,\n","    data_loader: DataLoader,\n",") -> tp.Tuple[ntp.NDArray[np.int_], ntp.NDArray[np.int_]]:\n","    model.eval()\n","\n","    y_true, logits  = [], []\n","    with torch.no_grad():\n","        iterator = iter(data_loader)\n","        for _ in tqdm(range(len(data_loader))):\n","            try:\n","                batch_X, batch_y = next(iterator)\n","            except:\n","                continue\n","\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","\n","            output = model(batch_X)\n","\n","            logits.extend(output.detach().cpu().numpy())\n","            y_true.extend(batch_y.detach().cpu().numpy())\n","\n","    return np.vstack(y_true), np.vstack(logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T21:35:41.194832Z","iopub.status.busy":"2023-02-15T21:35:41.193700Z","iopub.status.idle":"2023-02-15T21:35:41.207971Z","shell.execute_reply":"2023-02-15T21:35:41.206739Z","shell.execute_reply.started":"2023-02-15T21:35:41.194774Z"},"trusted":true},"outputs":[],"source":["import itertools\n","from sklearn.metrics import f1_score, fbeta_score, accuracy_score, precision_score, recall_score\n","from functools import partial\n","\n","def get_scorers() -> tp.List[tp.Tuple[str, tp.Callable[[ntp.NDArray[np.int_], ntp.NDArray[np.int_]], np.float_]]]:\n","    return [\n","        ('F1 (micro)', partial(f1_score, average='micro', zero_division=0)),\n","        ('F1 (macro)', partial(f1_score, average='macro', zero_division=0)),\n","        ('F1 (samples)', partial(f1_score, average='samples', zero_division=0)),\n","        ('F2 (micro)', partial(fbeta_score, beta=2, average='micro', zero_division=0)),\n","        ('F2 (macro)', partial(fbeta_score, beta=2, average='macro', zero_division=0)),\n","        ('F2 (samples)', partial(fbeta_score, beta=2, average='samples', zero_division=0)),\n","        ('Accuracy', accuracy_score),\n","        ('Precision', partial(precision_score, average='macro', zero_division=0)),\n","        ('Recall', partial(recall_score, average='macro', zero_division=0)),\n","    ]\n","\n","def evaluate(y_true: ntp.NDArray[np.int_], logits: ntp.NDArray[np.int_]) -> pd.DataFrame:\n","    scorers, data = get_scorers(), {}\n","    for i in tqdm(range(2500)):\n","        thresholds = np.random.uniform(low=0.0, high=0.5, size=17)\n","        \n","        y_pred = (logits > thresholds).astype(int)\n","\n","        scores = []\n","        for _, scorer in scorers:\n","            scores.append(scorer(y_true, y_pred))\n","\n","        data[i] = scores + thresholds.tolist()\n","\n","    return pd.DataFrame.from_dict(data, columns=[name for name, _ in scorers] + [f'Thresh #{i + 1}' for i in range(17)], orient='index')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T21:35:41.211975Z","iopub.status.busy":"2023-02-15T21:35:41.211525Z","iopub.status.idle":"2023-02-15T21:39:10.629420Z","shell.execute_reply":"2023-02-15T21:39:10.628316Z","shell.execute_reply.started":"2023-02-15T21:35:41.211934Z"},"trusted":true},"outputs":[],"source":["y_true, outputs = predict(model, device, dataloader_test)# itertools.islice(dataloader_test, 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T21:39:10.632399Z","iopub.status.busy":"2023-02-15T21:39:10.631790Z","iopub.status.idle":"2023-02-15T21:39:10.641319Z","shell.execute_reply":"2023-02-15T21:39:10.640404Z","shell.execute_reply.started":"2023-02-15T21:39:10.632358Z"},"trusted":true},"outputs":[],"source":["logits = 1/(1 + np.exp(-outputs))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T21:39:10.643446Z","iopub.status.busy":"2023-02-15T21:39:10.642721Z","iopub.status.idle":"2023-02-15T22:17:51.578357Z","shell.execute_reply":"2023-02-15T22:17:51.577220Z","shell.execute_reply.started":"2023-02-15T21:39:10.643406Z"},"trusted":true},"outputs":[],"source":["results = evaluate(y_true, logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T22:17:51.581638Z","iopub.status.busy":"2023-02-15T22:17:51.581260Z","iopub.status.idle":"2023-02-15T22:17:51.610999Z","shell.execute_reply":"2023-02-15T22:17:51.609936Z","shell.execute_reply.started":"2023-02-15T22:17:51.581600Z"},"trusted":true},"outputs":[],"source":["results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T22:17:51.612933Z","iopub.status.busy":"2023-02-15T22:17:51.612548Z","iopub.status.idle":"2023-02-15T22:17:51.620401Z","shell.execute_reply":"2023-02-15T22:17:51.619334Z","shell.execute_reply.started":"2023-02-15T22:17:51.612894Z"},"trusted":true},"outputs":[],"source":["[0.16, 0.06, 0.17, 0.13, 0.11, 0.12, 0.05, 0.1, 0.16, 0.23, 0.14, 0.1, 0.16, 0.22, 0.28, 0.08, 0.13]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T22:17:51.622447Z","iopub.status.busy":"2023-02-15T22:17:51.622063Z","iopub.status.idle":"2023-02-15T22:17:51.648407Z","shell.execute_reply":"2023-02-15T22:17:51.647448Z","shell.execute_reply.started":"2023-02-15T22:17:51.622412Z"},"trusted":true},"outputs":[],"source":["results[results['F2 (samples)'] == results['F2 (samples)'].max()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T22:17:51.650226Z","iopub.status.busy":"2023-02-15T22:17:51.649748Z","iopub.status.idle":"2023-02-15T22:17:51.675097Z","shell.execute_reply":"2023-02-15T22:17:51.673943Z","shell.execute_reply.started":"2023-02-15T22:17:51.650184Z"},"trusted":true},"outputs":[],"source":["results[results['F1 (samples)'] == results['F1 (samples)'].max()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T22:17:51.677228Z","iopub.status.busy":"2023-02-15T22:17:51.676805Z","iopub.status.idle":"2023-02-15T22:17:51.816702Z","shell.execute_reply":"2023-02-15T22:17:51.815144Z","shell.execute_reply.started":"2023-02-15T22:17:51.677189Z"},"trusted":true},"outputs":[],"source":["results.to_csv(BASE_DIR / 'resnet.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":".env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"vscode":{"interpreter":{"hash":"b8ee81d84a34806034cf616698f38de3e99e4e7e40364b23dcb85d774b869f2c"}}},"nbformat":4,"nbformat_minor":4}
