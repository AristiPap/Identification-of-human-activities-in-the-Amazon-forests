{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Exploration","metadata":{"id":"8AvFrxTUlVoV"}},{"cell_type":"markdown","source":"## Installing dependencies","metadata":{}},{"cell_type":"code","source":"DEPENDENCIES = [\n    'tf-slim==1.1.0',\n    'numpy==1.21.6',\n    'pandas==1.3.5',\n    'seaborn',\n    'torch==1.11.0',\n    'torchvision==0.12.0',\n    'matplotlib==3.5.3',\n    'opencv-python==4.5.4.60',\n    'sklearn==0.0.post1',\n    'skorch==0.12.1',\n    'tqdm',\n    'requests',\n    'plotly==5.11.0',\n    'scikit-image==0.19.3',\n]","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:14:02.162729Z","iopub.execute_input":"2023-01-12T14:14:02.163595Z","iopub.status.idle":"2023-01-12T14:14:02.187432Z","shell.execute_reply.started":"2023-01-12T14:14:02.163504Z","shell.execute_reply":"2023-01-12T14:14:02.186498Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import subprocess\nimport typing as tp\nimport re\n\ndef install_dependencies(dependencies: tp.List[str], show_progress: bool = True) -> tp.Tuple[tp.List[str], tp.List[Exception]]:\n    emit = print if show_progress else lambda x: None\n\n    resolved_dependencies, errors = [], []\n    for dependency in dependencies:\n        emit(f'Installing \"{dependency}\"...')\n\n        try:\n            subprocess.run([\"pip\", \"install\", \"--root-user-action=ignore\", dependency], stdout=subprocess.DEVNULL)\n            \n            if '==' in dependency:\n                dependency = re.search('(.+)==.+', dependency).group(1)\n\n            if '@' in dependency:\n                dependency = re.search('(.+) @ .+', dependency).group(1)\n            \n            pip_freeze = subprocess.Popen((\"pip\", \"freeze\"), stdout=subprocess.PIPE)\n            output = subprocess.check_output((\"grep\", \"-E\", f\"^({dependency}==)|({dependency} @).+$\"), stdin=pip_freeze.stdout)\n            resolved_dependencies.append(output.decode().strip())\n        except subprocess.CalledProcessError as e:\n            errors.append(e)\n    \n    return resolved_dependencies, errors","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:14:02.189517Z","iopub.execute_input":"2023-01-12T14:14:02.190262Z","iopub.status.idle":"2023-01-12T14:14:02.201728Z","shell.execute_reply.started":"2023-01-12T14:14:02.190222Z","shell.execute_reply":"2023-01-12T14:14:02.200713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"install_dependencies(DEPENDENCIES)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:14:02.203330Z","iopub.execute_input":"2023-01-12T14:14:02.204056Z","iopub.status.idle":"2023-01-12T14:16:23.820313Z","shell.execute_reply.started":"2023-01-12T14:14:02.204009Z","shell.execute_reply":"2023-01-12T14:16:23.819035Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Installing \"tf-slim==1.1.0\"...\nInstalling \"numpy==1.21.6\"...\nInstalling \"pandas==1.3.5\"...\nInstalling \"seaborn\"...\nInstalling \"torch==1.11.0+cpu\"...\n","output_type":"stream"},{"name":"stderr","text":"ERROR: Could not find a version that satisfies the requirement torch==1.11.0+cpu (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)\nERROR: No matching distribution found for torch==1.11.0+cpu\n","output_type":"stream"},{"name":"stdout","text":"Installing \"torchvision==0.12.0+cpu\"...\n","output_type":"stream"},{"name":"stderr","text":"ERROR: Could not find a version that satisfies the requirement torchvision==0.12.0+cpu (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1)\nERROR: No matching distribution found for torchvision==0.12.0+cpu\n","output_type":"stream"},{"name":"stdout","text":"Installing \"matplotlib==3.5.3\"...\nInstalling \"opencv-python==4.5.4.60\"...\nInstalling \"sklearn==0.0.post1\"...\nInstalling \"skorch==0.12.1\"...\nInstalling \"tqdm\"...\nInstalling \"requests\"...\nInstalling \"plotly==5.11.0\"...\nInstalling \"scikit-image==0.19.3\"...\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(['tf-slim==1.1.0',\n  'numpy==1.21.6',\n  'pandas==1.3.5',\n  'seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-split_1629095986539/work',\n  'torch @ file:///tmp/torch/torch-1.11.0-cp37-cp37m-linux_x86_64.whl',\n  'torchvision @ file:///tmp/torch/torchvision-0.12.0-cp37-cp37m-linux_x86_64.whl',\n  'matplotlib==3.5.3',\n  'opencv-python==4.5.4.60',\n  'sklearn==0.0.post1',\n  'skorch==0.12.1',\n  'tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1649051611147/work',\n  'requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work',\n  'plotly==5.11.0',\n  'scikit-image==0.19.3'],\n [])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Seeding RNGs","metadata":{}},{"cell_type":"markdown","source":"Achieving reproducibility in our results, requires initializing (also known as `seeding`) the random number generators (RNG) utilized by our dependencies. In order to do so, we designate a `RANDOM_SEED` number, namely `1234`, and we use it to initialize the following RNGs:\n\n- `numpy` (`np.random.seed`)\n- `random` (`random.seed`)\n- `torch (CPU)` (`torch.manual_seed`)\n- `torch (GPU)` (`torch.cuda.manual_seed`)\n\nThe aforementioned RNGs are utilized by `torch`, `numpy` as well as `sklearn` in order to generate random numbers. `random.seed` corresponds to the python standard library RNG. We are seeding each and every one of them in order to cover any possible edge cases, wherein third party code utilizes any of them unbeknownst to us. Lastly, `PYTHONHASHSEED` controls the hashing of str, bytes and datetime objects. More specifically (as stated in the official `Python` documentation):\n\n_\"If this variable is not set or set to random, a random value is used to seed the hashes of str, bytes and datetime objects...\"_","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport random\n\nRANDOM_SEED = 1234\n\nif RANDOM_SEED is not None:\n    np.random.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    torch.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed(RANDOM_SEED)\n    os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.895005Z","iopub.execute_input":"2023-01-12T14:34:11.896033Z","iopub.status.idle":"2023-01-12T14:34:11.903397Z","shell.execute_reply.started":"2023-01-12T14:34:11.895963Z","shell.execute_reply":"2023-01-12T14:34:11.902269Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nBASE_DIR = Path.cwd()\nINPUT_DIR = Path(\"/\") / \"kaggle\" / \"input\"\nDATA_DIR = INPUT_DIR / \"planets-dataset\" / \"planet\" / \"planet\" # https://www.kaggle.com/datasets/nikitarom/planets-dataset\n\nTRAIN_SAMPLES_DIR = DATA_DIR / 'train-jpg'\nTRAIN_LABELS_FILE = DATA_DIR / 'train_classes.csv' ","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.905710Z","iopub.execute_input":"2023-01-12T14:34:11.906468Z","iopub.status.idle":"2023-01-12T14:34:11.915119Z","shell.execute_reply.started":"2023-01-12T14:34:11.906433Z","shell.execute_reply":"2023-01-12T14:34:11.913940Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"We define our custom Dataset class to manipulate batches of data between RAM and Disk more easily. Some point of attentions:\n\n- __init__: we pass the dataframe along with the target, the transformation, the file path and is_train flag. It is important to distinguish the training phase from the testing phase because we use test augmentation. Test augmentation (TTA) is helpful to diversify our training dataset and build a more robust model. It is applied on each image for each batch, meaning that is doesn't increase the length of our training dataset per say, but it transforms each image randomly during execution time.\n- __getitem__: we define what the dataset return upon iteration. It needs to load both image and target. collate_fn: we use this function within the following DataLoader instance. It corresponds to the batch manipulation. This is were transform is called. We also proceed to train and test augmentation there.\n- collate_fn: we use this function within the following DataLoader instance. It corresponds to the batch manipulation. This is were transform is called. We also proceed to train and test augmentation there.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport cv2\nimport torch\nimport numpy.typing as ntp\n\nTransform = tp.Callable[[torch.Tensor], torch.Tensor]\n\nclass AmazonDataset(Dataset):\n    def __init__(self, dataset_dir: Path, image_names: tp.List[str], tags: tp.List[tp.List[int]], transform: tp.Optional[Transform] = None) -> None:\n        super().__init__()\n\n        self.dataset_dir = dataset_dir\n        self.image_names = image_names\n        self.tags = tags\n        self.transform = transform\n\n    def __len__(self) -> int:\n        return len(self.image_names)\n\n    def __getitem__(self, idx: int) -> tp.Tuple[ntp.NDArray[np.float_], ntp.NDArray[np.int_]]:\n        image = self.load_image(idx)\n        tags = self.load_tags(idx)\n        \n        return image, tags\n\n    def load_tags(self, idx: int) -> torch.Tensor:\n        tags = self.tags[idx]\n        tags = torch.as_tensor(tags)\n        tags = tags.float()\n        \n        return tags\n    \n    def load_image(self, idx: int) -> torch.Tensor:\n        image_name = self.image_names[idx]\n        filename = f'{image_name}.jpg'\n        filepath = self.dataset_dir / filename\n\n        image = cv2.imread(str(filepath))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = torch.tensor(image)\n        image = image.permute(2, 0, 1)\n        image = self.transform(image)\n        image = image.float()\n        \n        return image","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.916767Z","iopub.execute_input":"2023-01-12T14:34:11.917186Z","iopub.status.idle":"2023-01-12T14:34:11.932255Z","shell.execute_reply.started":"2023-01-12T14:34:11.917153Z","shell.execute_reply":"2023-01-12T14:34:11.931410Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as T\n\ntransform_train = T.Compose([\n    T.ToPILImage(),\n    T.Resize(224),\n    T.ToTensor(),\n    T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    T.RandomHorizontalFlip(),\n    T.RandomRotation(180)\n])\n\ntransform_val = T.Compose([\n    T.ToPILImage(),\n    T.Resize(224),\n    T.ToTensor(),\n    T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n    T.RandomHorizontalFlip(),\n    T.RandomRotation(180)\n])","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.935154Z","iopub.execute_input":"2023-01-12T14:34:11.935778Z","iopub.status.idle":"2023-01-12T14:34:11.944158Z","shell.execute_reply.started":"2023-01-12T14:34:11.935742Z","shell.execute_reply":"2023-01-12T14:34:11.943345Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom torch.utils.data import DataLoader\n\n\ndef create_datasets(\n    dataset_dir: Path,\n    classes_filepath: Path,\n    batch_size: int = 64,\n    test_size: float = 0.2,\n    shuffle: bool = False,\n    limit: tp.Optional[int] = None\n) -> tp.Tuple[DataLoader, DataLoader, MultiLabelBinarizer]:\n    df = pd.read_csv(classes_filepath)\n    \n    if limit is not None:\n        df = df.head(limit)\n    \n    df.tags = np.char.split(df.tags.values.astype(str))\n    \n    df_train, df_val = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    encoder = MultiLabelBinarizer()\n    tags_train = encoder.fit_transform(df_train.tags)\n    tags_val = encoder.transform(df_val.tags)\n    \n    dataset_train = AmazonDataset(dataset_dir, df_train.image_name.to_numpy(), tags_train, transform_train)\n    dataset_val = AmazonDataset(dataset_dir, df_val.image_name.to_numpy(), tags_val, transform_val)\n\n    dataloader_train = DataLoader(\n      dataset_train,\n      batch_size=batch_size,\n      shuffle=True,\n    )\n\n    dataloader_val = DataLoader(\n      dataset_val,\n      batch_size=batch_size,\n      shuffle=True,\n    )\n\n    return dataloader_train, dataloader_val, encoder","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.945611Z","iopub.execute_input":"2023-01-12T14:34:11.946018Z","iopub.status.idle":"2023-01-12T14:34:11.958789Z","shell.execute_reply.started":"2023-01-12T14:34:11.945967Z","shell.execute_reply":"2023-01-12T14:34:11.957997Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"dataloader_train, dataloader_val, encoder = create_datasets(TRAIN_SAMPLES_DIR, TRAIN_LABELS_FILE)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:11.960345Z","iopub.execute_input":"2023-01-12T14:34:11.960864Z","iopub.status.idle":"2023-01-12T14:34:12.096141Z","shell.execute_reply.started":"2023-01-12T14:34:11.960829Z","shell.execute_reply":"2023-01-12T14:34:12.094920Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(f'Training set: {len(dataloader_train)}, Validation set: {len(dataloader_val)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:12.099445Z","iopub.execute_input":"2023-01-12T14:34:12.100086Z","iopub.status.idle":"2023-01-12T14:34:12.105143Z","shell.execute_reply.started":"2023-01-12T14:34:12.100055Z","shell.execute_reply":"2023-01-12T14:34:12.104051Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Training set: 506, Validation set: 127\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For optimal performances, resnet18 need input shape that are multiple of 32 and in our case we have input of size 256. From 256, the closest multiple of 32 is 224.\n\nTherefore, we rescale our input data using this multiple, and we also normalize our dataset based on resnet pretrained mean and standard deviation intensity values. ToTensor() is useful to normalize our image values from 0-255 range to 0-1 range.","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom torchvision import models\n\nclass ResNet(nn.Module):\n    def __init__(self, freeze: bool = True, dropout: float = 0.2):\n        super().__init__()\n\n        self.resnet18 = models.resnet18(pretrained=True)\n        for parameter in self.resnet18.parameters():\n            parameter.require_grad = not freeze\n        \n        self.resnet18.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n        self.resnet18.fc = nn.Sequential(\n          nn.Flatten(),\n          nn.Linear(512, 128), # 512 for resnet18 or 2048 for resnet 50\n          nn.ReLU(inplace=True),\n          nn.Dropout(dropout),\n          nn.Linear(128, 17),\n          nn.Sigmoid()\n        )\n\n    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n        return self.resnet18(batch)\n\n    @classmethod\n    def from_device(cls, *args, device_id=\"cpu\", **kwargs) -> tp.Tuple[\"ResNet\", torch.device]:\n        # We firstly initialize an instance of our model\n        model = cls(*args, **kwargs)\n\n        # If the cuda backend is available then change the device type to GPU\n        if torch.cuda.is_available():\n            device_id = \"cuda:0\"\n            # Given that there are multiple GPUs available wrap the model\n            # in `nn.DataParallel` in order to take advantage of them\n            if torch.cuda.device_count() > 1:\n                model = nn.DataParallel(model)\n\n        # Retrieve the `torch.device` corresponding to `device_id`\n        # and transfer the model to it\n        device = torch.device(device_id)\n\n        return model.to(device), device\n\n    @classmethod\n    def from_file(cls, filename: Path, *args, device_id: str = \"cpu\", **kwargs) -> tp.Tuple[\"ResNet\", torch.device]:\n        # Firstly initialize the model and retrieve the device wherein it is located\n        model, device = cls.from_device(*args, device_id=device_id, **kwargs)\n\n        # Load the model state from the supplied file\n        # and dynamically remap it to the device at hand using the `map_location`\n        model.load_state_dict(torch.load(filename, map_location=device))\n\n        return model, device","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:12.106908Z","iopub.execute_input":"2023-01-12T14:34:12.107450Z","iopub.status.idle":"2023-01-12T14:34:12.122977Z","shell.execute_reply.started":"2023-01-12T14:34:12.107416Z","shell.execute_reply":"2023-01-12T14:34:12.122061Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\nclass EarlyStoppingStrategy(object):\n    def __init__(self, \n        tolerance: int = 5,\n        min_delta: float = 0,\n        checkpoint_dir: tp.Optional[Path] = None,\n    ):\n        self.tolerance = tolerance\n        self.min_delta = min_delta\n        self.checkpoint_dir = checkpoint_dir\n\n        self.best_validation_loss = float('inf')\n        self.counter = 0\n\n    def __call__(self, validation_loss, model):\n        best_validation_loss = self.best_validation_loss\n        self.best_validation_loss = min(self.best_validation_loss, validation_loss)\n\n        if best_validation_loss - validation_loss < self.min_delta:\n            # if validation loss value at hand is not at least `min_delta`\n            # smaller than the so far smallest validation loss\n            # increment the tolerance counter by 1\n            # If the counter exceeds the specified tolerance level we should\n            # halt the training procedure\n            self.counter += 1\n            if self.counter > self.tolerance:\n                return True\n        else:\n            # Otherwise (meaning the validation loss has decreased considerably)\n            # reset the tolerance counter and persist the model state\n            self.counter = 0\n\n            if self.checkpoint_dir is not None:\n                filename = f'{model.__class__.__name__}_{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S_%f\")}.pkl'\n\n                torch.save(model.state_dict(), self.checkpoint_dir / filename)\n\n        return False","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:12.126282Z","iopub.execute_input":"2023-01-12T14:34:12.126621Z","iopub.status.idle":"2023-01-12T14:34:12.138021Z","shell.execute_reply.started":"2023-01-12T14:34:12.126572Z","shell.execute_reply":"2023-01-12T14:34:12.137137Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import time\n\nfrom torch import nn, optim\nfrom tqdm.notebook import tqdm\n\ndef train(\n    model: nn.Module,\n    device: torch.device,\n    train_loader: DataLoader,\n    validation_loader: DataLoader,\n    early_stopping_strategy: tp.Optional[EarlyStoppingStrategy] = None,\n    n_epochs: int = 100,\n    weight_decay: float = 0.0,\n    lr=0.001,\n    eps=1e-08,\n) -> tp.Tuple[int, ...]:\n    # Define a method to retrieve the tqdm progress bar postfix data\n    batch_index, train_losses, validation_losses = 0, [0], [0]\n    def get_postfix():\n        return {\n            'train': f'{train_losses[-1]:.3f}',\n            'validation': f'{validation_losses[-1]:.3f}',\n            'batch': f'{batch_index:02d} / {len(train_loader):02d}'\n        }\n\n    if early_stopping_strategy is not None:\n        def get_postfix():\n            return {\n                'train': f'{train_losses[-1]:.3f}',\n                'validation': f'{validation_losses[-1]:.3f}',\n                'tolerance': f'{early_stopping_strategy.counter}/{early_stopping_strategy.tolerance}',\n                'batch': f'{batch_index:02d} / {len(train_loader):02d}'\n            }\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay, lr=lr, eps=eps)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=[0.25 * n_epochs, 0.5 * n_epochs, 0.75 * n_epochs])\n\n    epochs_progress_bar = tqdm(range(n_epochs), desc='epochs', position=0)\n\n    timestamp = time.time()\n    for _ in epochs_progress_bar:\n        # We set the model to training mode\n        model.train()\n\n        train_loss = 0\n        for batch_index, (batch_X, batch_y) in enumerate(train_loader):\n            # Transfer the data to the available device backend (CPU/GPU)\n            batch_X, batch_y, = batch_X.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n\n            output = model(batch_X)\n            loss = criterion(output, batch_y)\n            loss.backward()\n\n            optimizer.step()\n\n            train_loss += loss.item() / len(train_loader)\n\n            epochs_progress_bar.set_postfix(**get_postfix())\n\n        train_losses.append(train_loss)\n\n        epochs_progress_bar.set_postfix(**get_postfix())\n\n        scheduler.step()\n        \n        # We set the model to evaluation mode\n        model.eval()\n        with torch.no_grad():\n            # Turn of gradient calculation\n            validation_loss = 0\n            for batch_X, batch_y in validation_loader:\n                batch_X, batch_y, = batch_X.to(device), batch_y.to(device)\n\n                output = model(batch_X)\n                loss = criterion(output, batch_y)\n\n                validation_loss += loss.item() / len(validation_loader)\n\n            validation_losses.append(validation_loss)\n\n            epochs_progress_bar.set_postfix(**get_postfix())\n            \n        # Invoke the early stopping strategy with the current validation loss\n        # in order to increment/reset its internal tolerance counter\n        # and determine whether or not to halt the training process\n        if early_stopping_strategy is not None:\n            if early_stopping_strategy(validation_losses[-1], model):\n                break\n    \n    return time.time() - timestamp, np.array(train_losses[1:]), np.array(validation_losses[1:])","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:12.141642Z","iopub.execute_input":"2023-01-12T14:34:12.142017Z","iopub.status.idle":"2023-01-12T14:34:12.157616Z","shell.execute_reply.started":"2023-01-12T14:34:12.141963Z","shell.execute_reply":"2023-01-12T14:34:12.156700Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = Path.cwd() / 'models'\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\n\nearly_stopping_strategy = EarlyStoppingStrategy(\n    tolerance=10,\n    min_delta=0.005,\n    checkpoint_dir=checkpoint_dir\n)\n\nmodel, device = ResNet.from_device()\n\n_, train_losses, validation_losses = train(\n    model, device,\n    dataloader_train, dataloader_val,\n    early_stopping_strategy=early_stopping_strategy, n_epochs=100, weight_decay=0.05, lr=1e-4\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T14:34:12.159141Z","iopub.execute_input":"2023-01-12T14:34:12.159494Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"epochs:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455dbb2a7f8c42b6b18a66b6f3e54b03"}},"metadata":{}}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ndef learning_curves(\n    train_losses,\n    validation_losses,\n    title: str = 'Loss per Epoch',\n    label_x: str = 'Epochs',\n    label_y: str = 'Loss',\n) -> None:\n    epochs = np.arange(max(len(train_losses), len(validation_losses)))\n\n    go.Figure(data=[\n            go.Scatter(name='Training', x=epochs, y=train_losses, mode='lines'),\n            go.Scatter(name='Validation', x=epochs, y=validation_losses, mode='lines'),\n    ]).update_layout(title=title, xaxis_title=label_x, yaxis_title=label_y).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curves(train_losses, validation_losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating our ResNet architecture","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\ndef evaluate(model: nn.Module, device: torch.device, data_loader: DataLoader):\n    model.eval()\n\n    correct = 0\n    ground_truth_labels  = []\n    predicted_labels = []\n    with torch.no_grad():\n        for batch_X, batch_y, batch_lengths in tqdmf(data_loader, desc='batches'):\n            batch_X, batch_y, batch_lengths = batch_X.to(device), batch_y.to(device), batch_lengths.to(device)\n\n            output = model(batch_X, batch_lengths)\n            \n            _, predictions = torch.max(output, 1)\n\n            predicted_labels.extend(predictions.detach().cpu().numpy())\n            ground_truth_labels.extend(batch_y.detach().cpu().numpy())\n            \n            correct += (predictions == batch_y).sum().item()\n\n    classes = np.unique(ground_truth_labels)\n\n    clf_report = classification_report(ground_truth_labels, predicted_labels, labels=classes, output_dict = True)\n    precision = precision_score(ground_truth_labels, predicted_labels, average='micro')\n    recall = recall_score(ground_truth_labels, predicted_labels, average='micro')\n    f1 = f1_score(ground_truth_labels, predicted_labels, average='micro')\n    \n    micro_metrics = [precision ,recall ,f1, float(len(predicted_labels))]\n    clf_report_df = pd.DataFrame.from_dict(clf_report)\n    clf_report_df['micro avg'] = micro_metrics\n\n    return clf_report_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}